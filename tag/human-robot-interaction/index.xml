<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Human-Robot Interaction | Zijian Hu</title>
    <link>https://www.zijianhu.com/tag/human-robot-interaction/</link>
      <atom:link href="https://www.zijianhu.com/tag/human-robot-interaction/index.xml" rel="self" type="application/rss+xml" />
    <description>Human-Robot Interaction</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 31 May 2020 00:00:00 -0800</lastBuildDate>
    <image>
      <url>https://www.zijianhu.com/images/icon_hub8aaec6c2c2e1082c2b24756ea87d425_171065_512x512_fill_lanczos_center_2.png</url>
      <title>Human-Robot Interaction</title>
      <link>https://www.zijianhu.com/tag/human-robot-interaction/</link>
    </image>
    
    <item>
      <title>Can I Trust You? A User Study of Robot Mediation of a Support Group</title>
      <link>https://www.zijianhu.com/publication/birmingham-icra-2020/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/publication/birmingham-icra-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trust in Multi-Party Human-Robot Interaction</title>
      <link>https://www.zijianhu.com/project/multi_party/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/multi_party/</guid>
      <description>&lt;p&gt;In this project, we designed and evaluated a novel framework for robot mediation of a support group.
We conducted a user study using an NAO robot mediator controlled by a human operator that is unseen by the participants (&lt;a href=&#34;https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wizard-of-Oz&lt;/a&gt;).
At the end of each study, the participants are asked to annotate their trust towards other participants in the study session recordings.
In a &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;second-author paper&lt;/a&gt; at International Conference on Robotics and Automation (ICRA),
we showed that using a robot could significantly increase the average interpersonal trust after the group interaction session.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=multiparty_support&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;study-design&#34;&gt;Study Design&lt;/h2&gt;






  



  
  











&lt;figure id=&#34;figure-volunteers-demonstrating-the-study-setup&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_2000x2000_fit_q75_lanczos.jpg&#34; data-caption=&#34;Volunteers demonstrating the study setup&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_2000x2000_fit_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;4032&#34; height=&#34;3024&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Volunteers demonstrating the study setup
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In each study session, three participants were seated around the end of a table with a seated &lt;a href=&#34;https://www.softbankrobotics.com/emea/index.php/en/nao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAO&lt;/a&gt; robot as shown in Figure 1.
The NAO robot, acting as a group moderator, was positioned towards the participants.
On the table, a 360-degree microphone and 3 cameras facing directly to the participants&#39; face were placed.
Behind the robot, an RGB-D camera was mounted on a tripod to record the interactions between the group members.
The robot operator was seated behind a one-way mirror hidden from participants.&lt;/p&gt;
&lt;p&gt;To measure how the level of trust changes overtime, the participants were asked to report their trust towards other participants against the recordings of the current session after the group interaction.&lt;/p&gt;
&lt;p&gt;The detail for the procedure of the study can be found &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed NAO control program&lt;/li&gt;
&lt;li&gt;Designed and implemented web-based Wizard of Oz controller&lt;/li&gt;
&lt;li&gt;Designed and implemented self-annotation website&lt;/li&gt;
&lt;li&gt;Developed data collection program for one depth camera, three webcams and one 360 degree microphone&lt;/li&gt;
&lt;li&gt;Data post-processing for data whitening and fast data loading&lt;/li&gt;
&lt;li&gt;Turn-taking prediction with &lt;a href=&#34;http://zpascal.net/cvpr2017/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Temporal Convolutional Networks (TCN)&lt;/a&gt; and LSTM for multi-modal input&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;







  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/chris-birmingham/&#34;&gt;Chris Birmingham&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/kartik-mahajan/&#34;&gt;Kartik Mahajan&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/eli-reber/&#34;&gt;Eli Reber&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. Matarić&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;Can I Trust You? A User Study of Robot Mediation of a Support Group&lt;/a&gt;.
  &lt;em&gt;2020 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.04671&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/birmingham-icra-2020/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/multi_party/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/ICRA40945.2020.9196875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  


</description>
    </item>
    
    <item>
      <title>Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</guid>
      <description>&lt;h2 id=&#34;linked-material&#34;&gt;&lt;strong&gt;Linked material&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;YouTube video for &amp;ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&amp;rdquo;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Telepresence Robot for K-12 Remote Education</title>
      <link>https://www.zijianhu.com/project/nri_kids/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/nri_kids/</guid>
      <description>&lt;p&gt;In this project, we developed and evaluated various control methods and interfaces for mobile remote presence robots (MRP) for remote K-12 education. In the two papers published at the International Symposium on Robot and Human Interactive Communication (RO-MAN), we conducted a user study and evaluated our system.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=nrikids&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;We used an &lt;a href=&#34;https://ohmnilabs.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ohmni&lt;/a&gt; robot equipped with an arm and a Linux PC.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions consist of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed customized web user interface for robot arm control&lt;/li&gt;
&lt;li&gt;Designed and implemented communication protocol and software between the Linux PC and the Ohmni server&lt;/li&gt;
&lt;li&gt;Developed user interface with a turning dial for robot arm control&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;
&lt;p&gt;






  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/naomi-t.-fitter/&#34;&gt;Naomi T. Fitter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/youngseok-joung/&#34;&gt;Youngseok Joung&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/marton-demeter/&#34;&gt;Marton Demeter&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. Matarić&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-hardware/&#34;&gt;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;http://robotics.usc.edu/publications/1048/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-hardware/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  







  
  
    
  
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://youtu.be/Ft8XCAIbslE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  









  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/naomi-t.-fitter/&#34;&gt;Naomi T. Fitter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/youngseok-joung/&#34;&gt;Youngseok Joung&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/marton-demeter/&#34;&gt;Marton Demeter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. Matarić&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-ui/&#34;&gt;User Interface Tradeoffs for Remote Deictic Gesturing&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;http://robotics.usc.edu/publications/1049/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-ui/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>User Interface Tradeoffs for Remote Deictic Gesturing</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resource Distribution in Human-Robot Teams</title>
      <link>https://www.zijianhu.com/project/robot_team/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/robot_team/</guid>
      <description>&lt;p&gt;Our work looks to understand how robots can be successfully integrated into human teams. Much work in the Human-Robot Interaction space has investigated one on one interactions with one robot and one human. Our work looks to fill this gap of knowledge by providing an algorithm that takes into account the social construct of human fairness and optimization through a Multi Armed Bandit variant algorithm. We apply this algorithm to a robot tasked with distributing resources to different human team members&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designed and implemented multi-threaded perception and control system for &lt;a href=&#34;https://anki.com/en-ca/vector.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anki Vector&lt;/a&gt; robot&lt;/li&gt;
&lt;li&gt;Camera calibration with ArUco Markers&lt;/li&gt;
&lt;li&gt;ArUco Marker detection for navigation and robot localization&lt;/li&gt;
&lt;li&gt;Path planning using finite state machine&lt;/li&gt;
&lt;li&gt;Resource distribution with Multi Armed Bandit variant algorithm&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Infant-Robot Interaction as an Early Intervention Strategy</title>
      <link>https://www.zijianhu.com/project/baby/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/baby/</guid>
      <description>&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=babies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Infants engage in motor babbling that allows them to explore their space and learn what movements produce
desired outcomes. Less motor babbling from infants can lead to developmental delays.
Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide
contingent positive feedback to increase exploration and expand early movement practice.&lt;/p&gt;
&lt;p&gt;Towards this end, we are collaborating with physical therapists to create approaches to predict the
developmental status of infants using wearable sensors; running user studies that explore various robot
rewards for contingent activities for the infant, as well as measuring the infant&amp;rsquo;s ability to mimic the
robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to
increase the infant&amp;rsquo;s engagement with the task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PqTkw2weVjU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting and Tracking two &lt;a href=&#34;https://www.sphero.com/sphero-sprk-plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sphero SPRK+&lt;/a&gt; robots with a wall-mounted camera&lt;/li&gt;
&lt;li&gt;Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset&lt;/li&gt;
&lt;li&gt;Visual Tracking:
&lt;ul&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/abs/1812.11703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SiamRPN&lt;/a&gt;: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/pdf/1611.08461.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSRT tracker&lt;/a&gt;: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
